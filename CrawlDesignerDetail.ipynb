{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReadMe - Instruction\n",
    "###Before running code\n",
    "#1.make sure your python installed requests,beautifulsoup4 (type the following code inside terminal)\n",
    "#pip install -U requests\n",
    "#pip install -U beautifulsoup4\n",
    "\n",
    "#2.make sure you have \"query_result.csv\" (which contains column \"uname\" and \"url\") inside directory. If no, please do the following\n",
    "#Go to Gdrive -->RMBI4980 --> Kickstarter --> new --> \"Kickstarter code.sql\"; run it inside MySQL/Sequel Pro\n",
    "#Type query: select id,uname,name,url from top100; run it and export as \"query_result.csv\"\n",
    "\n",
    "###Instruction\n",
    "#This coding is used to crawl web data of all selected Kickstarter products from Kickstarter :\n",
    "#-->Loop all URL of the product\n",
    "#--> Crawl info of designer/product \n",
    "#--> Save all result in csv called \"DesignerData.csv\"\n",
    "\n",
    "#Remark: there is the function called randomSleep which is used to try avoiding to be blocked by the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###List of importing\n",
    "#For function \"make_soup\", \"crawl_info\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#For function \"rawdata\", Data_write_csv\", Data_append_csv\"\n",
    "import csv\n",
    "\n",
    "#For function \"rawdata\"\n",
    "from collections import defaultdict\n",
    "\n",
    "#For function \"randomSleep\"\n",
    "import time, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawdata():\n",
    "    columns = defaultdict(list) # each value in each column is appended to a list\n",
    "\n",
    "    with open('query_result.csv',encoding=\"latin-1\") as f:\n",
    "        reader = csv.DictReader(f) # read rows into a dictionary format\n",
    "        for row in reader: # read a row as {column1: value1, column2: value2,...}\n",
    "            for (k,v) in row.items(): # go over each column name and value \n",
    "                columns[k].append(v) # append the value into the appropriate list\n",
    "                                     # based on column name k\n",
    "\n",
    "    return(columns['uname'],columns['url'])\n",
    "\n",
    "#OR using Pandas, below is just for reference, may have some typo\n",
    "#import pandas as pd\n",
    "#data = pd.read_csv(\"sdfds.csv\")\n",
    "#uname = data['uname'] #pandas.series\n",
    "#uname = pd.to_list(uname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    return BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSleep():\n",
    "        sleeptime =  random.randint(2, 5)\n",
    "        time.sleep(sleeptime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_info(ProductName,KickstarterURL): \n",
    "    try:\n",
    "        ###Collect designer page of the product\n",
    "        soup=make_soup(KickstarterURL+'/creator_bio')\n",
    "        \n",
    "        ###Try avoiding to be blocked, with a random short wait\n",
    "        randomSleep()\n",
    "\n",
    "        ###For testing\n",
    "        #print(soup) \n",
    "        #print(soup.status_code) \n",
    "        #print(soup.prettify()) #show all coding in the web page\n",
    "        \n",
    "        links=[] #for store related websites\n",
    "        Data=[] #for store result\n",
    "        CompanyName = \"\"\n",
    "        CompanyDescription = \"\"\n",
    "        ListOfWebsites = \"\"       \n",
    "        MainDesignerName = \"\"\n",
    "        LastLogin = \"\"\n",
    "        LastLogin2 = \"\"\n",
    "        DesignerFB = \"\"\n",
    "        NoOfFBFriends = \"\"\n",
    "        NoOfProductCreated = \"\"\n",
    "        NoOfProductBacked = \"\"\n",
    "        NoOfUpdate = \"\"\n",
    "        StartFundingDate = \"\"\n",
    "        StartFundingDate2 = \"\"\n",
    "        EndFundingDate = \"\"\n",
    "        EndFundingDate2 = \"\"\n",
    "        \n",
    "        ##Below: before getting web data, we assume they are nothing, if the specific text is found, we will update them\n",
    "        #1. Crawl company name\n",
    "        CN = soup.find('div',class_=\"table-cell full-width px3 border-box\")\n",
    "        if CN:\n",
    "            CompanyName = CN.find('a',class_=\"green-dark\").get_text()\n",
    "            CompanyName = CompanyName.replace('\\n','') #remove \"\\n\" in the text\n",
    "        \n",
    "        #2. Crawl company description\n",
    "        CD = soup.find('div',class_=\"col col-7 col-post-1 pt3 pb3 pb10-sm\")\n",
    "        if CD:\n",
    "            CompanyDescription = CD.find('div',class_=\"readability\").get_text()\n",
    "            CompanyDescription = CompanyDescription.replace('\\n','') #remove \"\\n\" in the text\n",
    "        \n",
    "        #3. Crawl list of related websites (e.g. company homepage, facebook, twitter)\n",
    "        LW = soup.find('div',class_=\"pt3 pt7-sm mobile-hide\")\n",
    "        if LW:\n",
    "            ListOfWebsites = LW.findAll('a')\n",
    "\n",
    "            for link in ListOfWebsites:\n",
    "                links.append(link.get('href'))  #OR: links.append(link['href'].strip())\n",
    "        \n",
    "        ###Prepare for the following crawlling\n",
    "        RangeOfDesignerDetail = soup.find('div',class_=\"creator-bio-details col col-4 pt3 pb3 pb10-sm\")\n",
    "        \n",
    "        if RangeOfDesignerDetail:\n",
    "            #4. Crawl designer name\n",
    "            MDN = RangeOfDesignerDetail.find('span',class_=\"identity_name\")\n",
    "            if MDN:\n",
    "                MainDesignerName = MDN.get_text()\n",
    "                MainDesignerName = MainDesignerName.replace('\\n','') #remove \"\\n\" in the text\n",
    "\n",
    "            #5. Crawl last login\n",
    "            LL = RangeOfDesignerDetail.find('time')\n",
    "            if LL:\n",
    "                LastLogin = LL.get('datetime') #Original datetime\n",
    "\n",
    "                LastLogin2 = LL.get_text() #Adjusted time\n",
    "\n",
    "            #6. Crawl private FB account of designer\n",
    "            FB = RangeOfDesignerDetail.find('span',class_=\"number f6 nowrap\")\n",
    "            if FB:\n",
    "                DesignerFB = FB.find('a').get('href')\n",
    "\n",
    "            #7. Crawl facebook like of designer private account\n",
    "                NoOfFBFriends = FB.get_text().split()[0] #remove \"friends\" after the number\n",
    "\n",
    "            #8. Crawl number of product created by designer on Kickstarter\n",
    "            NPC = RangeOfDesignerDetail.find('div',class_=\"created-projects py2 f5 mb3\")\n",
    "            if NPC:\n",
    "                NoOfProductCreated = NPC.get_text().split()[0] #remove \"created\" after the number\n",
    "                NoOfProductCreated = NoOfProductCreated.replace('First','1') #change \"first\" into \"1\" if designer only create one product in Kickstarter\n",
    "\n",
    "                #9. Crawl number of product backed by designer on Kickstarter\n",
    "                NoOfProductBacked = NPC.get_text().split()[-2] #remove \"created\" after the number      \n",
    "\n",
    "        ###change soup to crawl the following data\n",
    "        soup=make_soup(KickstarterURL)\n",
    "\n",
    "        #10. Crawl number of time to update product's related info in Kickstarter\n",
    "        NOU = soup.find('a',class_=\"js-load-project-content js-load-project-updates mx3 project-nav__link--updates tabbed-nav__link type-14\")\n",
    "        if NOU:\n",
    "            NoOfUpdate = NOU.get_text().split()[1]\n",
    "\n",
    "        #11. Crawl funding period\n",
    "        period = soup.find('div',class_=\"NS_campaigns__funding_period\").find('p',class_=\"f5\").findAll()\n",
    "        if period:\n",
    "            StartFundingDate = period[0].get('datetime')\n",
    "            StartFundingDate2 = period[0].get_text()     \n",
    "\n",
    "            EndFundingDate = period[1].get('datetime')\n",
    "            EndFundingDate2 = period[1].get_text()\n",
    "        \n",
    "        Data.append(ProductName) #input value\n",
    "        Data.append(CompanyName)\n",
    "        Data.append(CompanyDescription)\n",
    "        Data.append(links)\n",
    "        Data.append(MainDesignerName)\n",
    "        Data.append(StartFundingDate)   #Data = Data + [StartFundingDate, StartFundingDate2]\n",
    "        Data.append(StartFundingDate2)\n",
    "        Data.append(EndFundingDate)\n",
    "        Data.append(EndFundingDate2)\n",
    "        Data.append(LastLogin)\n",
    "        Data.append(LastLogin2)\n",
    "        Data.append(NoOfUpdate)\n",
    "        Data.append(DesignerFB)\n",
    "        Data.append(NoOfFBFriends)\n",
    "        Data.append(NoOfProductCreated)\n",
    "        Data.append(NoOfProductBacked)\n",
    "        \n",
    "        Data_append_csv(Data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#crawl_info('naked-0','https://www.kickstarter.com/projects/1218200025/naked-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_write_csv():\n",
    "    with open(\"DesignerData.csv\",\"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file,delimiter=',')\n",
    "        writer.writerow([\"Product Name\",\"Company Name\",\"Company Description\",\\\n",
    "                         \"List of related websites\",\"Designer\",\"Start Funding Period\",\"Start Funding Period2\",\\\n",
    "                         \"End Funding Period\",\"End Funding Period2\",\"Last Login Time\",\\\n",
    "                         \"Adjusted Last Login Time\",\"No. of update for the product\",\\\n",
    "                         \"Facebook Account of designer\",\"No. of Facebook Friends\",\\\n",
    "                         \"No. of product created in Kickstarter\",\"Number of product backed in Kickstarter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_append_csv(data):\n",
    "    with open(\"DesignerData.csv\",\"a\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'find'\n",
      "'NoneType' object has no attribute 'find'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    #Collect list of product name and their Kickstarter URL from \"query_result.csv\"\n",
    "    ListOfProductName = rawdata()[0]\n",
    "    ListOfKickstarterURL = rawdata()[1]\n",
    "    \n",
    "    #Create a new csv file called \"DesignerData.csv\" storing result of data crawling\n",
    "    Data_write_csv() \n",
    "    \n",
    "    x=0\n",
    "    \n",
    "    for KickstarterURL in ListOfKickstarterURL:\n",
    "        #1. Crawl comment from website & 2. put them into \"DesignerData.csv\" file\n",
    "        ProductName = ListOfProductName[x]\n",
    "        \n",
    "        crawl_info(ProductName,KickstarterURL)\n",
    "\n",
    "        x +=1 #change to next product\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
